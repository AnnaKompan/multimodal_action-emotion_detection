{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573eb346",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95c58e",
   "metadata": {},
   "source": [
    "## 2. Initialize MediaPipe modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1806cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model \n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities \n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c88d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     conda create -n my_gpu_env python=3.x\n",
    "#     conda activate my_gpu_env\n",
    "\n",
    "# conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45b222",
   "metadata": {},
   "source": [
    "## 3. Define Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model): \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.setflags(write=False)         # Image is no longer writable\n",
    "    results = model.process(image)      # Make prediction\n",
    "    image.setflags(write=True)          # Image is now writable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c9871",
   "metadata": {},
   "source": [
    "## 4. Drawing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),  \n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                              )\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c93ac8",
   "metadata": {},
   "source": [
    "## 5. Main Video Capturing Logic \n",
    "(show-no need for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Main function \n",
    "# cap = cv2.VideoCapture(0) \n",
    "# # Set mediapipe model  \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic: \n",
    "#     while cap.isOpened(): \n",
    "  \n",
    "#         # Read feed \n",
    "#         ret, frame = cap.read() \n",
    "  \n",
    "#         # Make detections \n",
    "#         image, results = mediapipe_detection(frame, holistic) \n",
    "#         print(results) \n",
    "          \n",
    "#         # Draw landmarks \n",
    "#         draw_styled_landmarks(image, results) \n",
    "  \n",
    "#         # Show to screen \n",
    "#         cv2.imshow('Holistic Model Output', image) \n",
    "  \n",
    "#         # Break gracefully \n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'): \n",
    "#             break\n",
    "#     cap.release() \n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaab3a6",
   "metadata": {},
   "source": [
    "## 6. Extract Keypoint Values \n",
    "(x,y,z values from detected body + store in numpy arr or \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same logic as above but in function, so can be used\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]) # flatten arr of points x,y,z visibility values - single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test f(x) if correct num\n",
    "# extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test f(x) if correct num\n",
    "# len(results.face/pose/lh/rh_landmarks.landmark) = \n",
    "# face model has 468*3 + pose(33*4) + lh/rh(21*3 and 21*3)\n",
    "468*3+33*4+21*3+21*3 #total keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346ca7f",
   "metadata": {},
   "source": [
    "## 7. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#var that holds path for exported data (numpy arrays)\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['idontknow', 'areyouok', 'idontunderstand'])\n",
    "\n",
    "# 30 videos worth of data (sequence of data)\n",
    "no_sequences = 50 # 50 * 1662 keypoints to detect actions\n",
    "\n",
    "# Videos are going to be 50 frames in length\n",
    "sequence_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f2c5c",
   "metadata": {},
   "source": [
    "no need to rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1 folder for each action(3) and inside 1 folder for each sequence(0-29)\n",
    "# for action in actions: \n",
    "#     for sequence in range(no_sequences):\n",
    "#         try: \n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "#         except: # if created -> pass\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a64d84",
   "metadata": {},
   "source": [
    "## 8. Collect Keypoint Values for Training and Testing\n",
    "no need to re-run (have collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2731ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "#     # NEW LOOP\n",
    "#     # Loop through actions\n",
    "#     for action in actions:\n",
    "#         # Loop through sequences aka videos\n",
    "#         for sequence in range(no_sequences):\n",
    "#             # Loop through video length aka sequence length\n",
    "#             for frame_num in range(sequence_length):\n",
    "\n",
    "#                 # Read feed\n",
    "#                 ret, frame = cap.read()\n",
    "\n",
    "#                 # Make detections\n",
    "#                 image, results = mediapipe_detection(frame, holistic)\n",
    "# #                 print(results)\n",
    "\n",
    "#                 # Draw landmarks\n",
    "#                 draw_styled_landmarks(image, results)\n",
    "                \n",
    "#                 # NEW Apply wait logic\n",
    "#                 if frame_num == 0: \n",
    "#                     cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "#                     cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     # Show to screen\n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "#                     cv2.waitKey(2000)\n",
    "#                 else: \n",
    "#                     cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     # Show to screen\n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "#                 # NEW Export keypoints\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "#                 np.save(npy_path, keypoints) #to save 'npy_path.npy' array of resulting keypoints\n",
    "\n",
    "#                 # Break gracefully\n",
    "#                 if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                     break\n",
    "                    \n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8b650",
   "metadata": {},
   "source": [
    "## 9. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical # to one-hot encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "# label dictionary to represent each one of our action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185784ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2adffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences, labels = [], [] #x, y data\n",
    "# for action in actions:\n",
    "#     for sequence in range(no_sequences):\n",
    "#         window = []\n",
    "#         for frame_num in range(sequence_length):\n",
    "#             res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "#             window.append(res) #grab frame, add to window(video)\n",
    "#         sequences.append(window)\n",
    "#         labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d007690",
   "metadata": {},
   "source": [
    "## With augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# SEED = 42\n",
    "# np.random.seed(SEED)\n",
    "# random.seed(SEED)\n",
    "# torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750828a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying augmentation to each frame\n",
    "def augment_landmarks(frame, apply_prob=0.5): # probability of applying <50%\n",
    "    augmented = frame.copy()\n",
    "\n",
    "    if np.random.rand() < apply_prob:\n",
    "        # Gaussian\n",
    "        augmented = augmented + np.random.normal(0, 0.01, augmented.shape)\n",
    "\n",
    "    # if np.random.rand() < apply_prob:\n",
    "    #     # Scale\n",
    "    #     scale = np.random.uniform(0.9, 1.1)\n",
    "    #     augmented = augmented * scale\n",
    "\n",
    "    # if np.random.rand() < apply_prob:\n",
    "    #     # Translation (shifting)\n",
    "    #     shift = np.random.uniform(-0.05, 0.05, augmented.shape)\n",
    "    #     augmented = augmented + shift\n",
    "\n",
    "    # if np.random.rand() < apply_prob:\n",
    "    #     if augmented.ndim == 2 and augmented.shape[1] >= 2:\n",
    "    #         theta = np.random.uniform(-10, 10) * np.pi / 180\n",
    "    #         rotation_matrix = np.array([\n",
    "    #             [np.cos(theta), -np.sin(theta)],\n",
    "    #             [np.sin(theta), np.cos(theta)]\n",
    "    #         ])\n",
    "    #         augmented[:, :2] = augmented[:, :2] @ rotation_matrix\n",
    "    \n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation on sequence of frames (50)\n",
    "def augment_sequence(windows, apply_prob=0.3, target_len=50):\n",
    "    augmented = np.array(windows).copy()\n",
    "    # if np.random.rand() < apply_prob:\n",
    "    #     # Drop random fram\n",
    "    #     drop_idx = np.random.randint(0, len(augmented))\n",
    "    #     augmented = np.delete(augmented, drop_idx, axis=0)\n",
    "\n",
    "    if np.random.rand() < apply_prob:\n",
    "        shift = np.random.randint(-2,2)\n",
    "        augmented = np.roll(augmented, shift=shift, axis=0)\n",
    "    \n",
    "    if np.random.rand() < apply_prob:\n",
    "        # Speed up/Slow down\n",
    "        factor = np.random.choice([0.9, 1.1])\n",
    "        idxs = np.linspace(0, len(augmented) - 1, int(len(augmented) * factor))\n",
    "        idxs = np.clip(np.round(idxs), 0, len(augmented) - 1).astype(int)\n",
    "        augmented = augmented[idxs]\n",
    "\n",
    "    if len(augmented) < target_len:\n",
    "        pad_width = ((0, target_len - len(augmented)), (0, 0))\n",
    "        augmented = np.pad(augmented, pad_width, mode='constant')\n",
    "    elif len(augmented) > target_len:\n",
    "        augmented = augmented[:target_len]\n",
    "\n",
    "    return augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b82946",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_sequences, augmented_labels = [], []\n",
    "for seq, label in zip(sequences, labels):\n",
    "    augmented_sequences.append(seq)\n",
    "    augmented_labels.append(label)\n",
    "    \n",
    "    if np.random.rand() < 0.3:\n",
    "        augmented_sequences.append(augment_sequence(seq))\n",
    "        augmented_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa295312",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], [] #x, y data\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "\n",
    "            # Frame augmentation (Gaussian/scal/translation + drop/shift/)\n",
    "            if np.random.rand() < 0.3:\n",
    "                res = augment_landmarks(res)\n",
    "            window.append(res) #grab frame, add to window(video)\n",
    "        \n",
    "        # Sequence augmentation (Gaussian/scal/translation + drop/shift/speed up/slow down)\n",
    "        if np.random.rand() < 0.3:\n",
    "            window = augment_sequence(window)\n",
    "\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7072e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb02bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd68e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int) #to 1 hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15c70b",
   "metadata": {},
   "source": [
    "## 10. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential #sequential api\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard #to monitor model as its training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "RANDOM_SEED = \n",
    "OPTIMIZER = 'Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8aa3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(50,1662)))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9f4f8",
   "metadata": {},
   "source": [
    "## Simplify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(50,1662)))\n",
    "model.add(LSTM(32, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac97af7",
   "metadata": {},
   "source": [
    "categorical_crossentropy -> multi-class classification\n",
    "\n",
    "binary_crossentropy -> binary-class classification\n",
    "\n",
    "MSE -> regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a5a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=400,\n",
    "    callbacks=[tb_callback]  # TensorBoard callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f6753",
   "metadata": {},
   "source": [
    "In Terminal move to Logs/train: \n",
    "\n",
    "tensorboard --logdir=. (to see logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=. \n",
    "# to see logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['categorical_accuracy'], label='train_accuracy')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e084223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "# LSTM not CNN layers, cuz CNN needs more data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb074cad",
   "metadata": {},
   "source": [
    "##  Plot Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_landmarks(original, augmented, num_points=543):\n",
    "    if (original.ndim == 1):\n",
    "        num_points = original.size // 3\n",
    "        original = original.reshape((num_points, 3))\n",
    "        augmented = augmented.reshape((num_points, 3))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(original[:,0], original[:,1], c='blue', s=10)\n",
    "    plt.title(\"Original\")\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(augmented[:,0], augmented[:,1], c='red', s=10)\n",
    "    plt.title(\"Augmented\")\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = np.load('../ActionDetectionforSignLanguage/MP_Data/areyouok/0/2.npy')\n",
    "aug_frame = augment_landmarks(frame)\n",
    "plot_landmarks(frame, aug_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d24df",
   "metadata": {},
   "source": [
    "## 11. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f876ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[1])] #train data val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[1])] #pred of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817417d",
   "metadata": {},
   "source": [
    "## 12. Save Model and Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba8e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('400_simple_augm_mod.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e99d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('400_simple_augm_mod.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2290ca4",
   "metadata": {},
   "source": [
    "# 13. Evaluation using Confusion Matrix and Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "# whats being detected as TP FN FP TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ac46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40cdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=1 -> want to convert second dim in the arr\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32635c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat # numbers of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c645a55",
   "metadata": {},
   "source": [
    "multilabel_confusion_matrix:\n",
    "1) True Positive\n",
    "2) False Positive\n",
    "3) False Negative\n",
    "4) True Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21630afa",
   "metadata": {},
   "source": [
    "# 14. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f to render probabilities\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)] \n",
    "# color combo for each action: hello, thanks, iloveyou\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40),\n",
    "                    (int(prob*100), 90+num*40),\n",
    "                    colors[num], -1\n",
    "        ) \n",
    "        # bar dynamically changes based on probability (longer = higher)\n",
    "        cv2.putText(\n",
    "            output_frame, actions[num],\n",
    "            (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1, (255,255,255),\n",
    "            2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d02c56",
   "metadata": {},
   "source": [
    "# 15. Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = [] # append 50 frames, once got 50 frames, predict\n",
    "sentence = [] # concatenate history of detection together\n",
    "predictions = []\n",
    "threshold = 0.3\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.4, min_tracking_confidence=0.4) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints) \n",
    "        # sequence = sequence[:50] # but still taking first 50 frames, although added to the end\n",
    "        sequence = sequence[-50:]\n",
    "        \n",
    "        if len(sequence) == 50:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0] #pass 1 sequence at a time\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res)) # append all preds to prediction arr\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): # grab only last 10 preds and only unique\n",
    "                if res[np.argmax(res)] > threshold: #check if res above threshold 0.5\n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]: #check if current action isn't same as last sentence\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1) # top corner, size, color, filled rect\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA) \n",
    "        # render sentence with space between (3,30 starting position)(font, font size, font color, font line width, line type)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X_test[0]) # shape is incorrect ERROR!!!!\n",
    "X_test[0].shape # model expects (num_sequences, 50 1662) -> expand dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b03639",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(X_test[0], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(X_test[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[np.argmax(res)] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a055d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(X_test[0], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864bc447",
   "metadata": {},
   "source": [
    "## After training Sign Language Model -> Train Emotion Rec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9f461",
   "metadata": {},
   "source": [
    "### Sign Language DATASET:\n",
    "- ASL Dataset\n",
    "- WLASL (Word-Level American Sign Language)\n",
    "\n",
    "### Emotion DATASET:\n",
    "- FER2013\n",
    "- AffectNet\n",
    "- CK+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Capture frame from webcam or video\n",
    "frame = get_video_frame()\n",
    "\n",
    "# 2. Extract landmarks using MediaPipe\n",
    "results = holistic.process(frame)\n",
    "\n",
    "# 3. Get input for sign model (hands + pose keypoints)\n",
    "sign_input = extract_sign_keypoints(results)\n",
    "\n",
    "# 4. Get input for emotion model (face crop or landmarks)\n",
    "face_crop = extract_face_image(frame, results)\n",
    "\n",
    "# 5. Predict sign\n",
    "sign_pred = sign_model.predict(sign_input)\n",
    "\n",
    "# 6. Predict emotion\n",
    "emotion_pred = emotion_model.predict(face_crop)\n",
    "\n",
    "# 7. Display predictions\n",
    "display(frame, sign=sign_pred, emotion=emotion_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
